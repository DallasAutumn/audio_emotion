<!DOCTYPE html>
    <html>
    <head>
        <meta charset="UTF-8">
        <title>author：段秋阳</title>
        
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Microsoft/vscode/extensions/markdown-language-features/media/markdown.css">
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Microsoft/vscode/extensions/markdown-language-features/media/highlight.css">
        
        <style>
.task-list-item { list-style-type: none; } .task-list-item-checkbox { margin-left: -20px; vertical-align: middle; }
</style>
        <style>
            body {
                font-family: -apple-system, BlinkMacSystemFont, 'Segoe WPC', 'Segoe UI', 'Ubuntu', 'Droid Sans', sans-serif;
                font-size: 14px;
                line-height: 1.6;
            }
        </style>
        
        
    </head>
    <body class="vscode-light">
        <h1 id="author%e6%ae%b5%e7%a7%8b%e9%98%b3">author：段秋阳</h1>
<h2 id="%e7%9b%ae%e7%9a%84%e5%bb%ba%e7%ab%8b%e4%b8%80%e5%a5%97%e8%87%aa%e5%8a%a8%e5%8c%96%e7%b3%bb%e7%bb%9f%e6%8e%a5%e6%94%b6%e4%ba%ba%e5%a3%b0%e5%b9%b6%e5%88%a4%e6%96%ad%e6%83%85%e7%bb%aabonus%e6%83%85%e6%84%9f%e8%af%ad%e9%9f%b3%e7%94%9f%e6%88%90">目的：建立一套自动化系统，接收人声并判断情绪。（bonus：情感语音生成）</h2>
<h2 id="%e6%96%b9%e6%b3%95%e6%b5%81%e7%a8%8b">方法流程：</h2>
<ol>
<li>梅尔倒频谱（mfcc）提取音频特征，作为模型的输入特征。</li>
<li>BiLSTM时间序列，或CNN对spectrogram图像进行分类，从而情感分类。</li>
</ol>
<h2 id="%e4%bd%bf%e7%94%a8%e5%b7%a5%e5%85%b7">使用工具：</h2>
<ol>
<li>pytorch torchaudio</li>
<li>librosa</li>
</ol>
<h2 id="%e5%b7%b2%e6%9c%89%e7%9a%84%e7%bb%93%e6%9e%9c%e6%80%bb%e7%bb%93">已有的结果总结：</h2>
<ul>
<li>数据的一系列预处理，包括提取mfcc特征，输入模型所做的准备如padding</li>
<li>CNN conv2d training</li>
<li><a href="https://github.com/DallasAutumn/audio_emotion">https://github.com/DallasAutumn/audio_emotion</a></li>
</ul>
<h2 id="%e5%8f%82%e8%80%83%e8%b5%84%e6%96%99">参考资料：</h2>
<ol>
<li><a href="https://towardsdatascience.com/speech-emotion-recognition-with-convolution-neural-network-1e6bb7130ce3">https://towardsdatascience.com/speech-emotion-recognition-with-convolution-neural-network-1e6bb7130ce3</a></li>
<li><a href="https://mp.weixin.qq.com/s?__biz=MzI2NjkyNDQ3Mw==&amp;mid=2247491361&amp;idx=1&amp;sn=d17ba628792975e2497124d292f68573&amp;chksm=ea87e7f7ddf06ee19eaf8af843d0ab9889ab0b688a1331c027b7a7229aa0f1cab81234661078&amp;scene=0&amp;xtrack=1&amp;key=6d90834972a32f5a2caf22fc167d13564caaf0905b0892813a7a3756c1d25d52d6446f5030b7cfd2936126361304860c26347aa7c2a5398a8758afefe135f045853c9a26543344382a87a6258e445c41&amp;ascene=14&amp;uin=MjMxNDI3NDQ2&amp;devicetype=Windows+10&amp;version=62060834&amp;lang=zh_CN&amp;pass_ticket=VamF2RkimJKpiIT5chpM2XY%2BUrlCnHj2uj7QHDfQO6Y%3D">https://mp.weixin.qq.com/s?__biz=MzI2NjkyNDQ3Mw==&amp;mid=2247491361&amp;idx=1&amp;sn=d17ba628792975e2497124d292f68573&amp;chksm=ea87e7f7ddf06ee19eaf8af843d0ab9889ab0b688a1331c027b7a7229aa0f1cab81234661078&amp;scene=0&amp;xtrack=1&amp;key=6d90834972a32f5a2caf22fc167d13564caaf0905b0892813a7a3756c1d25d52d6446f5030b7cfd2936126361304860c26347aa7c2a5398a8758afefe135f045853c9a26543344382a87a6258e445c41&amp;ascene=14&amp;uin=MjMxNDI3NDQ2&amp;devicetype=Windows+10&amp;version=62060834&amp;lang=zh_CN&amp;pass_ticket=VamF2RkimJKpiIT5chpM2XY%2BUrlCnHj2uj7QHDfQO6Y%3D</a></li>
<li><a href="https://pytorch.org/audio/">torchaudio documentation</a></li>
<li><a href="http://practicalcryptography.com/miscellaneous/machine-learning/guide-mel-frequency-cepstral-coefficients-mfccs/">MFCC tutorial</a></li>
</ol>

    </body>
    </html>